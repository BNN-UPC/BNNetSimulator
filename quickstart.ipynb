{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example solution Walkthrough\n",
    "\n",
    "The following example will take you through the steps to generate a dataset for your network simulation. We recommend using this as a template for your own implementation and customizing it to suit your specific needs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Generation\n",
    "To generate the dataset, we will first need to define the graph topology, routing paths, and traffic matrix for each sample. These parameters will be used by the simulator to calculate the delay, jitter, and drops for each path.\n",
    "\n",
    "To begin, we will define the graph topology, including the nodes and edges that make up the graph, as well as the scheduling policy and buffer size for each node. We will then create a routing file that defines the paths between the nodes in the topology.\n",
    "\n",
    "Next, we will generate the traffic matrix, which includes information on the source and destination nodes, average bandwidth, time distribution, packet size and frequency, and ToS for each flow.\n",
    "\n",
    "Once we have defined these parameters, we can run the simulation and collect performance metrics such as delay, jitter, and drops for each path.\n",
    "\n",
    "If you need more information on the parameters of the dataset, check out the [input_parameters_glossary.ipynb](input_parameters_glossary.ipynb) notebook, which provides a detailed explanation of each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define destination for the generated samples\n",
    "training_dataset_path = \"training\"\n",
    "#paths relative to data folder\n",
    "graphs_path = \"graphs\"\n",
    "routings_path = \"routings\"\n",
    "tm_path = \"tm\"\n",
    "# Path to simulator file\n",
    "simulation_file = os.path.join(training_dataset_path,\"simulation.txt\")\n",
    "# Name of the dataset: Allows you to store several datasets in the same path\n",
    "# Each dataset will be stored at <training_dataset_path>/results/<name>\n",
    "dataset_name = \"dataset-1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folders\n",
    "if os.path.isdir(training_dataset_path):\n",
    "    print (\"Destination path already exists. Files within the directory may be overwritten.\")\n",
    "else:\n",
    "    os.makedirs(os.path.join(training_dataset_path,graphs_path))\n",
    "    os.mkdir(os.path.join(training_dataset_path,routings_path))\n",
    "    os.mkdir(os.path.join(training_dataset_path,tm_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a graph topology file. The graphs generated have the following characteristics:\n",
    "- The network is able to process 3 ToS: 1,2,3\n",
    "- All nodes have buffer sizes of 32000 bits and WFQ scheduling. ToS 1 is assigned to first queue, and ToS 2 and 3 to second queue.\n",
    "- All links have bandwidths of 100000 bits per second\n",
    "'''\n",
    "def generate_topology(net_size, graph_file):\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Set the maximum number of ToS that will use the input traffic of the network\n",
    "    G.graph[\"levelsToS\"] = 3\n",
    "    \n",
    "    nodes = []\n",
    "    node_degree = []\n",
    "    for n in range(net_size):\n",
    "        node_degree.append(random.choices([2,3,4,5,6],weights=[0.34,0.35,0.2,0.1,0.01])[0])\n",
    "        \n",
    "        nodes.append(n)\n",
    "        G.add_node(n)\n",
    "        # Assign to each node the scheduling Policy\n",
    "        G.nodes[n][\"schedulingPolicy\"] = \"SP\"\n",
    "        # Assign ToS to scheduling queues.\n",
    "        # In this case we have two queues per port. ToS 1 is assigned to the first queue and ToS 2 and 3 to the second queue\n",
    "        G.nodes[n][\"tosToQoSqueue\"] = \"1;2,3\"\n",
    "        # Assign weights to each queue\n",
    "        G.nodes[n][\"schedulingWeights\"] = \"60, 40\"\n",
    "        # Assign the buffer size of all the ports of the node\n",
    "        G.nodes[n][\"bufferSizes\"] = 32000\n",
    "\n",
    "    finish = False\n",
    "    while True:\n",
    "        aux_nodes = list(nodes)\n",
    "        n0 = random.choice(aux_nodes)\n",
    "        aux_nodes.remove(n0)\n",
    "        # Remove adjacents nodes (only one link between two nodes)\n",
    "        for n1 in G[n0]:\n",
    "            if n1 in aux_nodes:\n",
    "                aux_nodes.remove(n1)\n",
    "        if len(aux_nodes) == 0:\n",
    "            # No more links can be added to this node - can not acomplish node_degree for this node\n",
    "            nodes.remove(n0)\n",
    "            if len(nodes) == 1:\n",
    "                break\n",
    "            continue\n",
    "        n1 = random.choice(aux_nodes)\n",
    "        G.add_edge(n0, n1)\n",
    "        # Assign the link capacity to the link\n",
    "        G[n0][n1][\"bandwidth\"] = 100000\n",
    "        \n",
    "        for n in [n0,n1]:\n",
    "            node_degree[n] -= 1\n",
    "            if (node_degree[n] == 0):\n",
    "                nodes.remove(n)\n",
    "                if (len(nodes) == 1):\n",
    "                    finish = True\n",
    "                    break\n",
    "        if finish:\n",
    "            break\n",
    "    if not nx.is_connected(G):\n",
    "        G = generate_topology(net_size, graph_file)\n",
    "        return G\n",
    "    \n",
    "    nx.write_gml(G,graph_file)\n",
    "    \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a file with the shortest path routing of the topology G\n",
    "'''\n",
    "def generate_routing(G, routing_file):\n",
    "    with open(routing_file,\"w\") as r_fd:\n",
    "        lPaths = nx.shortest_path(G)\n",
    "        for src in G:\n",
    "            for dst in G:\n",
    "                if src == dst:\n",
    "                    continue\n",
    "                path =  ','.join(str(x) for x in lPaths[src][dst])\n",
    "                r_fd.write(path+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Generate a traffic matrix file. We consider flows between all nodes in the newtork, each with the following characterstics\n",
    "- The average bandwidth ranges between 10 and max_avg_lbda\n",
    "- We consider three time distributions (in case of the ON-OFF policy we have on periods of 10 and off periods of 5)\n",
    "- We consider two packages distributions, chosen at random\n",
    "- ToS is assigned randomly\n",
    "'''\n",
    "def generate_tm(G, max_avg_lbda, traffic_file):\n",
    "    poisson = \"0\" \n",
    "    cbr = \"1\"\n",
    "    on_off = \"2,10,5\" #time_distribution, avg on_time exp, avg off_time exp\n",
    "    time_dist = [poisson,cbr,on_off]\n",
    "    \n",
    "    pkt_dist_1 = \"0,300,0.5,1700,0.5\" #genric pkt size dist, pkt_size 1, prob 1, pkt_size 2, prob 2\n",
    "    pkt_dist_2 = \"0,500,0.6,1000,0.2,1400,0.2\" #genric pkt size dist, pkt_size 1, prob 1, \n",
    "                                               # pkt_size 2, prob 2, pkt_size 3, prob 3\n",
    "    pkt_size_dist = [pkt_dist_1, pkt_dist_2]\n",
    "    tos_lst = [0,1,2]\n",
    "    \n",
    "    with open(traffic_file,\"w\") as tm_fd:\n",
    "        for src in G:\n",
    "            for dst in G:\n",
    "                avg_bw = random.randint(10,max_avg_lbda)\n",
    "                td = random.choice(time_dist)\n",
    "                sd = random.choice(pkt_size_dist)\n",
    "                tos = random.choice(tos_lst)\n",
    "                \n",
    "                traffic_line = \"{},{},{},{},{},{}\".format(\n",
    "                    src,dst,avg_bw,td,sd,tos)\n",
    "                tm_fd.write(traffic_line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We generate the files using the previously defined functions. This code will produce 100 samples where:\n",
    "- We generate 5 topologies, and then we generate 20 traffic matrices for each\n",
    "- The topology sizes range from 6 to 10 nodes\n",
    "- We consider the maximum average bandwidth per flow as 1000\n",
    "\"\"\"\n",
    "max_avg_lbda = 1000\n",
    "with open (simulation_file,\"w\") as fd:\n",
    "    for net_size in range (6,11):\n",
    "        #Generate graph\n",
    "        graph_file = os.path.join(graphs_path,\"graph_{}.txt\".format(net_size))\n",
    "        G = generate_topology(net_size, os.path.join(training_dataset_path,graph_file))\n",
    "        # Generate routing\n",
    "        routing_file = os.path.join(routings_path,\"routing_{}.txt\".format(net_size))\n",
    "        generate_routing(G, os.path.join(training_dataset_path,routing_file))\n",
    "        # Generate TM:\n",
    "        for i in range (20):\n",
    "            tm_file = os.path.join(tm_path,\"tm_{}_{}.txt\".format(net_size,i))\n",
    "            generate_tm(G,max_avg_lbda, os.path.join(training_dataset_path,tm_file))\n",
    "            sim_line = \"{},{},{}\\n\".format(graph_file,routing_file,tm_file)   \n",
    "            # If dataset has been generated in windows, convert paths into linux format\n",
    "            fd.write(sim_line.replace(\"\\\\\",\"/\"))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the input files for the simulator, we are ready to run the simulation and collect the performance metrics. To do this, we will use a Docker image that contains all the necessary tools and dependencies.\n",
    "\n",
    "The Docker image is saved on Dockerhub, which means that when running the \"docker run\" command for the first time, the image will be downloaded automatically. All you need to make sure is that your computer is connected to the internet.\n",
    "\n",
    "Once the image is downloaded, you can use the \"docker run\" command to start the simulation and pass in the input files as parameters. The simulator will then use these input files to calculate the delay, jitter, and drops for each path.\n",
    "\n",
    "It's worth noting that the use of a Docker container ensures that the simulation runs in a consistent environment, regardless of the host machine's operating system and dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we generate the configuration file\n",
    "import yaml\n",
    "\n",
    "conf_file = os.path.join(training_dataset_path,\"conf.yml\")\n",
    "conf_parameters = {\n",
    "    \"threads\": 6,# Number of threads to use \n",
    "    \"dataset_name\": dataset_name, # Name of the dataset. It is created in <training_dataset_path>/results/<name>\n",
    "    \"samples_per_file\": 10, # Number of samples per compressed file\n",
    "    \"rm_prev_results\": \"n\", # If 'y' is selected and the results folder already exists, the folder is removed.\n",
    "}\n",
    "\n",
    "with open(conf_file, 'w') as fd:\n",
    "    yaml.dump(conf_parameters, fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "def docker_cmd(training_dataset_path):\n",
    "    raw_cmd = f\"docker run --rm --mount type=bind,src={os.path.join(os.getcwd(),training_dataset_path)},dst=/data bnnupc/netsim:v0.1\"\n",
    "    terminal_cmd = raw_cmd\n",
    "    if os.name != 'nt': # Unix, requires sudo\n",
    "        print(\"Superuser privileges are required to run docker. Introduce sudo password when prompted\")\n",
    "        terminal_cmd = f\"echo {getpass()} | sudo -S \" + raw_cmd\n",
    "        raw_cmd = \"sudo \" + raw_cmd\n",
    "    return raw_cmd, terminal_cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the docker\n",
    "raw_cmd, terminal_cmd = docker_cmd(training_dataset_path)\n",
    "print(\"The next cell will launch docker from the notebook. Alternatively, run the following command from a terminal:\")\n",
    "print(raw_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible that the execution cell may not produce an output until it finishes running the simulation. In this case, you can check the status of the simulation using the logs feature in Docker Desktop.\n",
    "\n",
    "To do this, simply go to the \"Containers\" section, select the \"bnnupc/bnnetsimulator\" container, and then click on \"Logs\". This will give you access to the log file, which contains information about the progress of the simulation.\n",
    "\n",
    "The log file will contain one line per simulated sample, with the first value indicating the simulation line, and then \"Ok\" if the simulation finishes properly, or an error message if there were any issues. The log file is located at <training_dataset_path>/out.log.\n",
    "\n",
    "It is recommended to regularly check the log file to ensure that the simulation is progressing as expected. This will help you catch any issues early on and take the necessary steps to resolve them.\n",
    "\n",
    "Additionally, you should also check the log file after the simulation has finished to ensure that there were no errors or other issues that may have affected the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{terminal_cmd}"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e4fabe4be1dcb5b95007215d13ed47b80f9ccf78939eea74ae4a681230c3cbef"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
